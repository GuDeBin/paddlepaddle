# 一个案例吃透深度学习

## 介绍

先把目录理一下

- 数据处理
- 网络结构
- 损失函数
- 优化算法
- 资源配置
- 训练调试与优化
- 恢复训练
- 动转静部署

而优化的点属于

- 样本
- 网络模型
- 损失函数
- 优化算法和学习率

而这四个基本上 paddle 都有相对应的直接的 api，也就是工具都有，流程也有，就是其中的参数和数据、业务需要你自己定义和梳理。

从教程的案例图片（一图抵千语，实在没有找到合适描述，侵删）
![“横纵式”教学法编写相当强大的模型](https://ai-studio-static-online.cdn.bcebos.com/140257def4fe4c6ea5ff5328ea6218873f6ec912dc2045beb472788b881825a1)

也能知道大致的流程，不算全部掌握，也算是基本了解，知道如果这个点我不理解，我知道它是什么范围的，也能在哪知道答案，也就是遇到不知道我知道我不知道，以及我知道如何知道。

## 第二周作业

### 客观题

---

1. 为了观察标签 Y 与 特征 X 之间的线性关系，使用哪种图形比较适合？

为了观察标签 Y 与 特征 X 之间的线性关系，适合使用散点图。散点图可以使我们快速识别数据中的趋势和离群值，因此可以帮助我们更好地了解标签 Y 与 特征 X 之间的关系。通过对散点图的观察，我们可以观察到数据点的分布情况，评估是否存在线性或非线性关系，并可以识别潜在的异常点或异常现象。

---

2. 继承 paddle.nn.Layer 实现自定义算子时，是否需要自定义 backward 函数

在继承 paddle.nn.Layer 实现自定义算子时，如果使用 PaddlePaddle 中的自动求导机制，则不需要自定义 backward 函数，因为在调用自定义算子时，Paddle 会自动为我们构建 grad_fn，采用动态图求导方法自动计算梯度。

当然，如果你想自定义 backward 函数，可以覆盖掉框架提供的默认实现来实现更复杂的梯度计算过程。但是这样做比较麻烦，需要手动计算每个输入值的梯度。通常情况下，使用自动求导机制，而不必自行计算每个梯度值，可以减少错误的发生和程序的复杂性，并且更容易调试和维护。

---

3. 在深度学习中，损失函数和评价指标是完全一致的？

不是完全一致的。

损失函数（loss function）与评价指标（metric）都是用来衡量模型预测结果与真实结果之间的差距，从而指导模型优化。但它们的不同之处在于，损失函数是用于反向传播计算梯度，优化模型参数，而评价指标是用于评估模型性能的标准，不参与模型的训练。

具体来说，损失函数在反向传播时被用于计算每个参数的梯度，通过梯度下降（或其他优化方法）调整参数以最小化损失函数，从而使模型的预测结果更接近真实值。而评价指标通常是一个指标函数，例如准确率、精确率、召回率等，用于评估模型预测的性能，比如模型是否能够正确地预测某种分类或回归问题。

总之，损失函数是用来训练模型的，而评价指标是用来评估模型的性能的标准。在深度学习中，我们通常会同时使用损失函数和评价指标，以便更好地监控模型的训练和测试性能。

---

4. 当输入和输出之间并不是线性关系时，可以定义非线性基函数对特征进行变换，从而可以使得线性回归算法实现非线性的曲线拟合

是的，当输入和输出之间不是线性关系时，可以使用非线性基函数对输入特征进行变换，从而使得线性回归模型可以实现非线性拟合。

具体而言，将非线性基函数应用于输入变量 X，可以将 X 转换成一个新的特征集合 Z，其中每个元素都是非线性基函数的输出。然后再将 Z 输入线性回归模型进行训练，从而得到一个非线性的回归模型。

例如，对于一个具有二次曲线形状的数据集，可以应用一个二次基函数来转换输入变量。具体而言，二次基函数可以将一个特征 x 转换为一个新的特征 z1 和 z2, 其中 $z_1=x$，$z_2=x^2$。这样，我们就可以把一个具有曲线形状的问题转换为一个线性回归问题，即用线性模型来拟合转换后的特征集合 Z，从而得到一个非线性拟合的结果。

总之，通过选择适当的非线性基函数，将输入特征进行变换和扩展，可以让线性回归算法拟合非线性数据，并提高预测的准确性。

---

5. 在回归问题中，通常选用均方误差作为优化目标，而不是绝对值误差，原因之一是绝对值误差在 0 处不可导

是的，均方误差通常被用作回归问题的优化目标，而不是绝对值误差，其中一个原因是绝对值误差在 0 处不可导。

在深度学习中，我们通常使用梯度下降法（或者其他的优化算法）来最小化损失函数。如果损失函数是连续可导的，则可以使用梯度下降法来计算最小值。但是，如果损失函数在某些点上不可导，则梯度下降法无法计算最小值。

对于绝对值误差，它在 0 处处不可导，因此在逐步迭代优化过程中，优化器无法更新梯度，这会导致更新速度缓慢或者停滞不前。

相比之下，均方误差函数是连续可导的，在逐步迭代优化过程中，它可以提供更平滑的更新方向。均方误差函数计算方便，而且有利于寻找梯度下降的最小点，因此在回归问题中通常被用作优化目标。

当然，有些情况下绝对值误差也是有用的，比如在 outlier detection 领域的一些任务中，它较为常用。

---

6. 使用交叉熵损失来量化预测分布和真实分布之间的差异。预测的概率分布与标签真实分布越接近，交叉熵越（）；越远，交叉熵越（）

使用交叉熵损失可以量化预测分布和真实分布之间的差异。具体而言，当预测的概率分布与标签真实分布越接近时，交叉熵损失越小；当它们之间越远时，交叉熵损失越大。

交叉熵损失在深度学习中广泛使用，例如在图像分类、自然语言处理、语音识别等任务中。比如，在图像分类中，我们使用 softmax 函数将原始输出转化为概率分布，然后将预测的概率分布与对应真实标签的分布进行比较，计算交叉熵损失。

交叉熵损失的优点是能够有效地惩罚错误的预测，比如如果将高概率分配给了错误的类别，则会产生较大的损失。相比之下，均方误差损失对于这种错误的惩罚更为低效。

总之，交叉熵损失是一种有效的损失函数，在深度学习中被广泛应用，尤其适用于多分类问题。

---

7. 对于一个数据量为 3 的二分类任务，假设预测的概率分布为[0.3,0.6,0.8]，真实标签为[0,0,1]，则准确率为

对于一个数据量为 3 的二分类任务，预测结果的概率分布为[0.3,0.6,0.8]，真实标签为[0,0,1]，要计算准确率，则需要将概率分布转化为类别预测结果。按照常规方法，将概率值大于等于 0.5 的类别视为 1，将概率值小于 0.5 的类别视为 0，则预测结果为[0,1,1]。接下来，我们可以与真实标签进行比较并计算准确率：

预测结果：[0,1,1]
真实标签：[0,0,1]
根据定义，准确率是指分类器预测正确的样本数与总样本数之比。根据上述预测结果和真实标签，共有 2 个样本被正确分类，因此准确率为：

准确率 = (预测正确的样本数) / (总样本数) = 2 / 3 ≈ 0.667

因此，该二分类任务的准确率为 0.667(约等于 0.67)。

---

8. 手写数字识别任务的数据处理环节通过调用 paddle.vision.datasets.MNIST 可以直接获取处理好的 MNIST 训练集和测试集。该 API 支持哪些数据集的读取

paddle.vision.datasets.MNIST 是 PaddlePaddle Vison 的一个 API 接口，用于获取处理好的 MNIST 数据集。除了 MNIST 数据集之外，paddle.vision.datasets 还支持多个其他数据集的读取。下面列举一下 paddle.vision.datasets 支持的数据集：

- MNIST
- FashionMNIST
- CIFAR10
- CIFAR100
- IMAGENET
- VOC
- COCO
- Flowers
- CelebA
- HMDB51
- UCF101
- Kinetics-400

这些数据集涵盖了计算机视觉、自然语言处理等领域的常见任务，例如图像分类、目标检测、图像分割、动作识别等。使用这些数据集可以帮助研究者更快地搭建模型并进行模型的训练和优化等操作。除了上述数据集，PaddlePaddle 也提供了更多其他数据集的支持，可以进一步了解一下官方的 API 文档。

---

9. 以上说法正确的有哪些

- 归一化是规范的一种；
- 归一化将数据缩放到[0,1]区间；
- 标准化属于规范化的一种；
- 标准化将每一维特征的分布规范为均值为 0，方差为 1；

以上四条说法都是正确的。具体解释如下：

归一化是规范的一种
归一化是数据预处理中的一种规范化方法，目的是将数据的取值范围限定在一定范围内，以避免不同特征之间取值范围的不同导致的特征重要性出入的问题。除了归一化，还有其他规范化方法，如标准化，将每一维特征的分布规范为均值为 0，方差为 1。

归一化将数据缩放到[0, 1]区间
归一化可以将数据缩放到[0, 1]区间，也可以缩放到其他区间，例如[-1, 1]、[0.5, 1]等。常用的缩放方式是将数据减去最小值，然后除以取值范围（最大值-最小值）。

标准化属于规范化的一种
标准化也是数据预处理中的一种规范化方法，与归一化不同的是，标准化通过将每一维特征经过简单的数学变换，将其转化为均值为 0，标准差为 1 的标准正态分布。标准化可以缩放数据的取值范围，同时还能够消除不同特征之间的量纲影响，以及将不同维度的特征分布平均化，凸显特征之间的差异。

标准化将每一维特征的分布规范为均值为 0，方差为 1
标准化会将数据标准化为均值为 0，标准差为 1 的标准正态分布，该分布可以轻松地比较不同特征之间的差异，并且差异不会因为量纲的差异而放大。标准化是处理数据中异常值的有力工具。

因此，以上四条说法都是正确的。

---

以上客观题均用 ChatGPT 回答，其中第八题错误，没有列出 VOC2012，但是已经很惊艳了，还有一题是问横纵式教育法的，很显然我没有 Chat GPT 好学。

### 实践题

列举出你基于该项目做出的具体优化，例如：

- 我将优化算法改成了 SGD，学习率 0.01，未能达到要求的 0.985
- 将优化算法改为 Momentum，学习率 0.01，达到要求，准确度 0.989
- 将优化算法改为 Adagrad，未能到达要求
- 将学习率改为 0.001，准确度下降
- 将 EPOCH_NUM 改为 10，准确度没有改变
