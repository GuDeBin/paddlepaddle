# 第四周学习

## 介绍

有点迷了，实在是不知道怎么搞下去，也就是在脱离课程之后该干什么

也就是，我知道现在 ai 的基本流程，也知道现在的范式是在预训练模型上微调

可是我能做什么，该做什么，想做什么

## 作业

### 实践题

本次作业内容为通过课上所学自然语言处理内容，实现观点抽取任务

### 客观题

---

Word2Vec 词向量训练算法中通过负采样解决了什么问题？

在 Word2Vec 词向量训练算法中，使用负采样能够解决两个问题：

- 提高训练效率：在 Skip-gram 模型中，每个中心词都需要更新与它相关的所有词的向量，包括它的上下文词和所有负样本。当词表很大时这个计算量相当大，会降低训练速度。通过负采样，我们只需要更新与中心词相关的几个负样本，大大提高了训练效率。

- 改善词向量质量：在 Skip-gram 模型中，对于每个中心词，我们需要计算它与所有上下文词的交叉熵损失，然后通过梯度下降来更新它的词向量。但是，在实际应用中，大多数上下文词与中心词是没有关联的，不需要更新它们的词向量。使用负采样将负样本中不相关的词去除，让训练过程只考虑与中心词相关的词，从而改善词向量质量。

---

一个序列展开长度为 n 的单层单向 RNN 模型，其输入维度为 input_size，隐状态向量维度为 hidden_size，则其可学习的参数量有多少？

单层单向 RNN 模型的可学习参数量包括三部分：

- 输入层到隐层的权重矩阵 W_ih，大小为（hidden_size, input_size），共有 hidden_size 个隐状态，每个隐状态与 input_size 个输入特征相连，因此参数量为 hidden_size \* input_size。

- 隐层到隐层的权重矩阵 W*hh，大小为（hidden_size, hidden_size），共有 n-1 个时序隐状态，每个隐状态都和上一个时序的隐状态相连，因此参数量为(hidden_size * hidden*size) * (n-1)。

- 隐层到输出层的权重矩阵 W_ho，大小为（output_size, hidden_size），假设输出维度为 output_size，则每个隐状态与 output_size 个输出特征相连，因此参数量为 output_size \* hidden_size。

另外，每个隐状态还有一个偏置项 b_h，大小为（hidden_size,），每个输出特征也有一个偏置项 b_o，大小为（output_size,）。因此，总的可学习参数量为：

hidden*size * input*size + (hidden_size * hidden*size) * (n-1) + output*size * hidden_size + hidden_size + output_size

注意，这个计算公式仅适用于标准的 RNN 模型，不包括 LSTM、GRU 等变种结构。

答案：(input_size + hidden_size) \*hidden_size + hidden_size

---

LSTM 模型中遗忘门负责什么功能？

LSTM（Long Short-Term Memory）模型是一种特殊的循环神经网络，为了解决传统 RNN 模型遇到的梯度消失和梯度爆炸等问题，引入了多个门控机制，其中遗忘门（forget gate）是其中一个重要的门控机制。

在 LSTM 模型中，遗忘门被用来控制前一时刻状态的遗忘程度。遗忘门的输入是前一时刻的状态和当前时刻的输入，它的输出是一个范围在 0 到 1 之间的值（减去 1 之后的值）。

当遗忘门的输出为 0 时，模型会完全遗忘前一时刻的状态信息，即前一时刻的状态全部清空；当遗忘门输出为 1 时，模型会完全保留前一时刻的状态信息。遗忘门通过学习得到最佳的权重，从而根据当前时刻输入中的需要保留或遗忘的信息，动态调整前一时刻状态被保留或遗忘的程度。

遗忘门的设计可以在一定程度上缓解梯度消失和梯度爆炸的问题，并且可以处理时间序列中长期依赖的情况，使得模型具备更强的表达能力。

---

Transformer 模型中 Encoder 和 Decoder 结构的主要区别是什么？

在 Transformer 模型中，Encoder 和 Decoder 结构虽然具有相同的基本构建单元（多头注意力机制和前馈神经网络），但是它们在具体实现上存在一些差异。

首先，在 Encoder 中，输入序列被嵌入到向量空间中，并经过一系列的多头注意力机制和前馈神经网络结构，最终生成了一个上下文向量。相比之下，Decoder 不仅包含了与 Encoder 相同的多头注意力机制和前馈神经网络结构，同时还引入了掩码注意力机制（Masked Multi-Head Attention）和 Encoder-Decoder 注意力机制（Encoder-Decoder Multi-Head Attention）。

其次，Encoder 和 Decoder 在注意力机制的使用上有所不同。在 Encoder 中，注意力机制的输入是来自上一个编码层的隐状态，帮助模型捕捉输入序列中的语义信息。而在 Decoder 中，则需要引入掩码机制来防止模型过度依赖当前时刻之后的信息，并使用 Encoder-Decoder 注意力机制来引入输入序列的信息，方便模型进行翻译或生成等任务。

最后，Encoder 和 Decoder 在模型训练过程中的输入序列和输出序列也存在一些差异。在 Encoder 中，输入序列是源语言序列，而 Decoder 的输入序列是目标语言序列。目标语言序列通过在序列的开始处添加一个特殊的起始标记（比如"s"），然后在每个时间步将前一时刻的输出作为当前时刻的输入，生成下一个符号的条件概率分布。

总之，虽然 Encoder 和 Decoder 之间存在一些差异，但它们共同构成了 Transformer 的基本框架，通过多头注意力机制和前馈神经网络，优雅地解决了序列到序列的任务，如机器翻译、对话生成等。

---

在 Transformer 中，自注意力计算出的 attention 分布为什么要除以根号 D？

在 Transformer 中使用多头自注意力机制，输入序列中的每个位置都能够和其他位置产生联系，即每个位置都需要计算 self-attention（自注意力），得到该位置所需要的注意力集中在哪些位置上。self-attention 的计算公式如下：

$$ \text { Attention } (Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d\_{k}}} \right) V $$

其中，$Q$, $K$ 和 $V$ 表示通过不同的线性变换得到的 $n \times d_k$ 矩阵，$n$ 表示序列长度，$d_k$ 表示对应的 $Q$, $K$ 或 $V$ 的向量维度。$softmax$ 函数沿着序列上的每个位置进行，并将 $n$ 个向量作为输入，输出具有相同长度和形状的 $n$ 个向量，表示在这些位置上的权重分布，每个位置上的权重和为 1。

注意力分布的计算大致可以理解成查询向量 $Q$ 与键向量 $K$ 的相似度（点积），再通过 $softmax$ 对相似度进行归一化。因此，为了控制相似度的大小，避免 softmax 的输入过大或过小导致导数消失，以及提高模型的稳定性，需要除以 $\sqrt{d_k}$。具体包含以下几个因素：

- 点积的结果越大，softmax 的输入越大，输出的分布越集中，对应位置注意力权重大致相等，位置间信息的携带能力就越强，但计算可能会不稳定。
- 在多头注意力中，每个注意力头的 $Q, K, V$ 矩阵的形状为 $d_{model} \times d_k$，其中 $d_{model}$ 表示全连接层的输出向量维度。$d_k$ 较小时，$Q$ 与 $K$ 的点积值较小，softmax 输出分布的熵较大，即分配给不同位置的权重差别较大，信息的分布也更为均匀。
- 在实际实验中发现，将 $\sqrt{d_k}$ 带入 softmax 中，可以让训练稳定更快地收敛，并且在一定程度上提高了模型的泛化能力。

综上所述，除以 $\sqrt{d_k}$ 是为了控制 softmax 输入的大小范围，提高模型训练的稳定性和泛化性。

---

在 Transformer 和 BERT 模型中，位置编码分别需要训练吗？

在 Transformer 和 BERT 中，位置编码是一种重要的技术，用于将输入序列中不同位置的特征进行区分。具体来说，它们需要将位置信息嵌入到输入特征中，使得相同的词在句子不同位置出现时能够得到不同的编码，从而避免忽略掉位置信息对句子表达的影响。

在 Transformer 中，位置编码由一组固定的向量表示，这些向量是在模型训练之前通过一定的数学公式计算得到的（如正弦余弦函数），并且对于不同的位置都是相同的，不需要进一步的训练。这是因为 Transformer 是一种序列到序列的模型，每个位置处的输入是相互独立的，位置编码只是为了区分不同位置的信息，不需要根据不同的输入序列进行调整。因此，在 Transformer 中，位置编码不需要进一步训练。

在 BERT 中，由于对于不同的句子输入，输入长度和位置信息都是不同的，因此位置编码需要重新训练。在 BERT 中，位置编码的计算方式和 Transformer 类似，但与 Transformer 不同的是，BERT 模型不是单向的，而是基于双向 Transformer 结构组成的。因此，在 BERT 中，位置编码需要针对不同的句子长度进行重新计算，并通过反向传播算法进行训练。实际上，BERT 中的位置编码是可学习的参数，模型可以通过训练调整位置编码的学习过程，以更好地适应不同长度的输入序列。

总之，在 Transformer 和 BERT 中，位置编码的含义和作用是相似的，但计算和训练方法略有不同。在 Transformer 中，位置编码是固定的，不需要训练，而在 BERT 中，位置编码是可训练的参数，需要针对不同的输入序列重新计算和学习。

---

还有哪些 RNN 模型的变体

除了 LSTM 和 GRU，还有一些其它的 RNN 变体，例如：

- Simple RNN：简单 RNN 是最基础的 RNN 形式，其中每个时间步的输出仅仅依赖于当前时间步的输入和上一个时间步的输出。

- Bidirectional RNN：双向 RNN 是通过在每个时间步上，同时考虑过去和未来的输入来增强 RNN 的表示能力。该模型中有两个独立的 RNN，分别处理正向序列和反向序列。

- Recurrent Convolutional Neural Network（RCNN）：RCNN 利用卷积神经网络来捕捉时序数据的局部相关性，然后将卷积层的输出作为输入传入 RNN 中，以捕捉全局的时序模式。RCNN 模型结合了 CNN 和 RNN 的优点，能够处理比较长的时间序列，同时保留了时序信息。

- Deep RNNs：深度 RNN 由多层序列模型堆叠而成，比浅层的 RNN 可以更好地建模复杂的序列模型，以提高准确性和性能。

- Clockwork RNN：时钟 RNN 将隐藏状态分解为多个模块，每个模块都对应一个特定的时间步长，以便在适当的时候更新该模块的状态。

- Hierarchical RNN：分层 RNN 将输入分层到不同的子序列中，并将其分别输入到不同的 RNN 层中处理，以利用更多的局部上下文信息。

总之，RNN 是一个十分灵活和广泛应用的模型族，其变种形式丰富多样，不同的变体可以针对不同的应用场景和需求，提供更好的性能和表达能力。

---

BERT 模型在训练过程中，使用了哪些预训练任务？

BERT 模型在训练过程中，使用了两个预训练任务：Masked Language Model（MLM）和 Next Sentence Prediction（NSP）。

- Masked Language Model（MLM）

MLM 的目标是随机遮盖输入文本中的 15%的单词，然后让模型来预测这些被遮盖的单词。当遮盖的单词是一个完整的 token 时，有 80%的概率用一个特殊的 token（"[MASK]"）来替换它，有 10%的概率用随机的单词替换它，有 10%的概率保留原单词不变。这个任务的目的在于让模型学会从上下文信息中推断缺失的词，并使得模型对输入文本的单词的分布和关联有更好的理解。

- Next Sentence Prediction（NSP）

NSP 的目标是判断两个句子之间是否具有先后顺序。具体地，在输入序列中，随机抽取两个句子，有 50%的概率选取两个连续的句子作为输入，这时模型需要预测它们是相邻的句子还是不相邻的句子；另外 50%的概率则选取两个不相邻的句子作为输入，模型需要预测它们是相邻的还是不相邻的。这个任务的目的在于让模型能够学习句子之间的语义关系。

通过这两个预训练任务，BERT 模型学习了一个通用的句子表示，能够更好地处理各种 NLP 任务，比如情感分析、语言推理、问答和文本生成等。

---

在 BERT 模型中，文本语义匹配的词嵌入模块包括以下哪些部分？

在 BERT 模型中，文本语义匹配的词嵌入模块包括以下两个部分：

- Segment Embeddings（表示句子序列）

Segment Embedding 是用来区分两个句子的，其目的是将不同的句子中的单词进行区分。对于输入的每个单词，BERT 模型为其分配一个 segment embedding，它可以告诉模型该单词来自哪个句子，并考虑到不同句子间的语义差异。这样可以让模型同时学习单词级别的语义和句子级别的语义。

- Position Embeddings（表示单词在句子中的位置）

Position embedding 是用来表示单词在句子中的位置，因为 BERT 模型没有使用 RNN，因此不能依靠 RNN 中存在的位置信息。Position embedding 使用了一种类似于 Sine 和 Cosine 函数的方法来表示单词在输入序列中的位置信息，将单词的位置信息嵌入向量中，然后再将其添加到 token embedding 的结果中，以捕捉单词之间位置的关系，这样就可以让模型为每个单词学习特定于位置的信息。

通过这两个部分结合，BERT 模型可以很好地表示单词和句子之间的语义关系和位置关系，从而提升文本语义匹配的性能。

答案：输入编码，分段编码和位置编码

---

小明最近在做中文情感分析的任务，经过深度的调研，小明觉得 PaddleNLP 集成了大量的预训练模型，同时具有很多代码样板可以参考，所以小明决定基于 PaddleNLP 集成的 BERT 模型进行中文语句级情感分类任务。请问小明可以怎样调用 BERT 模型实现这个任务呢？

在 PaddleNLP 中调用 BERT 模型进行中文语句级情感分析的任务，可以分为以下几个步骤：

安装 PaddlePaddle 和 PaddleNLP
在命令行中输入以下命令即可安装 PaddlePaddle 和 PaddleNLP：

```sh
pip install paddlepaddle -i https://mirror.baidu.com/pypi/simple
pip install paddlenlp -i https://mirror.baidu.com/pypi/simple
```

加载 BERT 预训练模型

在 PaddleNLP 中，可以通过 paddlenlp.transformers.BertModel.from_pretrained() 方法来加载预训练好的 BERT 模型。这个方法可以根据给定的 pretrained_model_name_or_path 自动选择相应的预训练模型，同时还可以选择是否加载预训练参数等。

```py
import paddle
import paddle.nn.functional as F
from paddlenlp.transformers import BertModel, BertTokenizer

# 加载BERT预训练模型和tokenizer
model = BertModel.from_pretrained('bert-base-chinese')
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
```

导入中文情感分类数据集
由于这里是进行情感分类，因此需要一个中文数据集用来训练和测试模型，可以使用 THUCNews 数据集或其他中文文本情感分类数据集。首先需要导入相关的 Python 模块，并从本地或网络中读取数据集，并预处理数据集。

```py
import random

# 读取数据集示例代码，此处伪造数据
def read_txt(txt_path):
    data = []
    with open(txt_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        for line in lines:
            line = line.strip()
            text, label = line.split('\t')
            data.append((text, label))
    return data

train_path = './train.txt'
eval_path = './dev.txt'
test_path = './test.txt'
train_data = read_txt(train_path)
eval_data = read_txt(eval_path)
test_data = read_txt(test_path)
label_list = ['0', '1']  # 情感分类的类别列表
```

数据预处理
PaddleNLP 还提供了一个内置的 SequentialSampler 和 DataLoader 工具，它将帮助我们对数据进行批量预处理、拆分、填充 mask 等，方便地将数据送入模型。使用 tokenizer 对训练数据进行处理，使其符合预训练模型的输入格式，同时使用 DataLoader 对数据进行处理。

```py
import os
from functools import partial
import numpy as np
from paddlenlp.data import Stack, Tuple, Pad
from paddlenlp.datasets import MapDataset
from paddlenlp.data import Sampler, DataLoader, BatchSampler

#将输入转化为数字id，增加分隔符、填充等操作。
trans_func = partial(
    convert_example,
    tokenizer=tokenizer,
    max_seq_length=128,
    is_test=False)

# 构造DataLoader
train_ds = MapDataset(
    train_data,
    lambda x: trans_func(x, label_list=label_list),
    lazy=True)

eval_ds = MapDataset(
    eval_data,
    lambda x: trans_func(x, label_list=label_list),
    lazy=True)

batch_size = 64
batchify_fn = Tuple(
    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # text_id
    Stack(dtype="int64"),  # seq_len
    Stack(dtype="int64")  # label_id
)

# 随机打乱样本数据
train_ds = train_ds.shuffle(buffer_size=len(train_ds))

# sort=False，顺序随机打乱，即不会按照数据集顺序一步步打乱，而是直接两两打乱，代表是无序打乱；sort=True，代表有序打乱，即将数据集按顺序划分，划分成block_size个块，每个块内数据不变，但每个块的顺序随机打乱，即有序打乱。
train_batch_sampler = BatchSampler(
    sampler=RandomSampler(train_ds),
    batch_size=batch_size,
    drop_last=False)
eval_batch_sampler = BatchSampler(
    sampler=SequentialSampler(eval_ds),
    batch_size=batch_size)

train_loader = DataLoader(
    dataset=train_ds,
    batch_sampler=train_batch_sampler,
    collate_fn=batchify_fn,
    return_list=True)
eval_loader = DataLoader(
    dataset=eval_ds,
    batch_sampler=eval_batch_sampler,
    collate_fn=batchify_fn,
    return_list=True)
```

定义情感分类模型
情感分类模型通常由 BERT 模型和一个分类器组成。在 PaddleNLP 中，可以创建一个新的神经网络，继承 paddle.nn.Layer 类，并在其中定义模型结构。

```py
class BertSentimentClassifier(paddle.nn.Layer):
    def __init__(self, bert, num_classes=2):
        super().__init__()
        self.bert = bert
```
